{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d092aa79-da78-499b-bd03-ffab81e438c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import vision_v1\n",
    "from google.protobuf import json_format\n",
    "import os\n",
    "import re\n",
    "\n",
    "key_path = 'KEY_FILE.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "for i in range(1, 8):\n",
    "    filename = f'bgReading_{i}.jpeg'\n",
    "    \n",
    "    with open(filename, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision_v1.Image(content=content)\n",
    "    image_context = vision_v1.ImageContext(language_hints=[\"en\"])\n",
    "\n",
    "    response = client.text_detection(image=image, image_context=image_context)\n",
    "    extracted_text = response.text_annotations\n",
    "\n",
    "    cleaned_text = \"\"\n",
    "    for text in extracted_text:\n",
    "        text_description = text.description\n",
    "        if isinstance(text_description, str):\n",
    "            cleaned_text += re.sub(r'[^\\w\\s./:]', '', text_description) + \" \"\n",
    "\n",
    "    text_in_one_line = \" \".join(cleaned_text.split())\n",
    "\n",
    "    with open('TextFromAccuChekImages.txt', 'a') as f:\n",
    "        f.write(text_in_one_line + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd1d99d-e857-46a9-a61f-b77f18d64869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Json file from the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea42d09d-ef53-4003-b542-fd28676fbe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "json_file_path = 'TextFromAccuChekImages.json'\n",
    "with open(json_file_path, \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c28f4e9-1da7-4b34-a0bb-74dd342b23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "def6640e-d36b-45de-bed3-5ccac21726cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "for example in data['annotations']:\n",
    "  temp_dict = {}\n",
    "  temp_dict['text'] = example[0]\n",
    "  temp_dict['entities'] = []\n",
    "  for annotation in example[1]['entities']:\n",
    "    start = annotation[0]\n",
    "    end = annotation[1]\n",
    "    label = annotation[2].upper()\n",
    "    temp_dict['entities'].append((start, end, label))\n",
    "  training_data.append(temp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce97596e-9d5e-4313-88fd-e93bd42a48d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'ACCUCHEK Guide Logbook 51 8:18am Fasting 10/29/20 mg/dL OK ACCU CHEK Guide Logbook 51 8:18 am Fasting 10/29/20 mg / dL OK', 'entities': [(0, 14, 'DEVICE NAME'), (23, 25, 'BG READING'), (26, 32, 'TIME'), (41, 49, 'DATE'), (50, 55, 'UNITS')]}\n"
     ]
    }
   ],
   "source": [
    "print(training_data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d50a39a0-4ae0-4941-9ae7-43450327d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17f15c20-70ef-45ab-9c66-5d4f06774033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 43/43 [00:00<00:00, 7421.41it/s]\n"
     ]
    }
   ],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example  in tqdm(training_data): \n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp.make_doc(text) \n",
    "    ents = []\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    filtered_ents = filter_spans(ents)\n",
    "    doc.ents = filtered_ents \n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"train.spacy\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8578eb2-daaf-491d-b3f1-52571633c139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f63b48e-3bf3-439d-9c60-3479fa636794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'ner']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     43.55    0.00    0.00    0.00    0.00\n",
      " 27     200         49.40   2000.64   96.68   96.32   97.04    0.97\n",
      " 60     400         42.94    401.00   98.18   96.43  100.00    0.98\n",
      "100     600        132.64    488.55   98.18   96.43  100.00    0.98\n",
      "150     800        149.28    551.10   98.18   96.43  100.00    0.98\n",
      "215    1000        782.41    975.44   98.18   96.43  100.00    0.98\n",
      "283    1200        140.66    683.19   97.42   97.06   97.78    0.97\n",
      "383    1400        115.99    917.43   98.18   96.43  100.00    0.98\n",
      "483    1600         78.04    898.77   98.18   96.43  100.00    0.98\n",
      "592    1800         72.27    943.29   98.18   96.43  100.00    0.98\n",
      "792    2000         94.70   1648.53   98.18   96.43  100.00    0.98\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy train config.cfg --output ./ --paths.train ./train.spacy --paths.dev ./train.spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "385f24c4-8950-4b05-be30-86c321e74abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Reading': [], 'units': [], 'date': [], 'time': [], 'deviceName': []}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from google.cloud import vision_v1\n",
    "import spacy\n",
    "\n",
    "key_path = 'KEY_FILE.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "def get_values_from_image(image_file_name):\n",
    "\n",
    "    # Read the image file\n",
    "    with open(image_file_name, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    # Create an image object\n",
    "    image = vision_v1.Image(content=content)\n",
    "    image_context = vision_v1.ImageContext(language_hints=[\"en\"])\n",
    "\n",
    "    # Perform text detection on the image\n",
    "    response = client.text_detection(image=image, image_context=image_context)\n",
    "    extracted_text = response.text_annotations\n",
    "\n",
    "    cleaned_text = \"\"\n",
    "    for text in extracted_text:\n",
    "        text_description = text.description\n",
    "        if isinstance(text_description, str):\n",
    "            cleaned_text += re.sub(r'[^\\w\\s./:]', '', text_description) + \" \"\n",
    "\n",
    "    one_line_text = \" \".join(cleaned_text.split())\n",
    "\n",
    "    # Initialize the spaCy NER model (assuming 'nlp_ner' is a valid NER model)\n",
    "    nlp_ner = spacy.load(\"model-best\")\n",
    "\n",
    "    # Process the one-line text with spaCy NER\n",
    "    doc = nlp_ner(one_line_text)\n",
    "\n",
    "    # Extract entities\n",
    "    readings = [ent.text for ent in doc.ents if ent.label_ == \"BG READING\"]\n",
    "    units = [ent.text for ent in doc.ents if ent.label_ == \"UNITS\"]\n",
    "    date = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "    time = [ent.text for ent in doc.ents if ent.label_ == \"TIME\"]\n",
    "    deviceName = [ent.text for ent in doc.ents if ent.label_ == \"DEVICE NAME\"]\n",
    "\n",
    "    # Create a dictionary to store the results\n",
    "    result = {\n",
    "        \"Reading\": readings,\n",
    "        \"units\": units,\n",
    "        \"date\": date,\n",
    "        \"time\": time,\n",
    "        \"deviceName\": deviceName\n",
    "    }\n",
    "\n",
    "    return result\n",
    "\n",
    "image_file_name = 'IMG_8431.HEIC'\n",
    "result = get_values_from_image(image_file_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7add9-a9f3-41c8-9d69-35ab8cc20fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Login with user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762b801-becb-4885-9805-a473e5cd4726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#upload data into it, add timestamp as date and time to local, add systimestamp as current timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083426a-f0b1-4e24-a7bf-f1a5a346049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if no reading return 'upload image again'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
