{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab4b7d73-b6de-4474-9d9d-33ef19a28bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import vision_v1\n",
    "from google.protobuf import json_format\n",
    "import os\n",
    "import re\n",
    "\n",
    "key_path = '/Users/pradeepsain/Downloads/KEY_FILE.json'\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "directory = '/Users/pradeepsain/Downloads/dash_img/'\n",
    "\n",
    "for i in range(1, 50):\n",
    "    filename = f'readingImg_{i}.jpg'\n",
    "    img_path = os.path.join(directory, filename)\n",
    "    \n",
    "    with open(img_path, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "    image = vision_v1.Image(content=content)\n",
    "    image_context = vision_v1.ImageContext(language_hints=[\"en\"])\n",
    "\n",
    "    response = client.text_detection(image=image, image_context=image_context)\n",
    "    extracted_text = response.text_annotations\n",
    "\n",
    "    cleaned_text = \"\"\n",
    "    for text in extracted_text:\n",
    "        text_description = text.description\n",
    "        if isinstance(text_description, str):\n",
    "            cleaned_text += re.sub(r'[^\\w\\s./:]', '', text_description) + \" \"\n",
    "\n",
    "    text_in_one_line = \" \".join(cleaned_text.split())\n",
    "\n",
    "    with open('/Users/pradeepsain/Downloads/TextFromImages.txt', 'a') as f:\n",
    "        f.write(text_in_one_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "270dda44-a262-4d45-a85f-cc33cbbdad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "json_file_path = '/Users/pradeepsain/Downloads/TextFromImages.json'\n",
    "with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "dataset = []\n",
    "\n",
    "annotations = data.get(\"annotations\", [])\n",
    "for entry in annotations:\n",
    "    if entry is not None:\n",
    "        text = entry[0]\n",
    "        entities = entry[1]['entities']\n",
    "\n",
    "        dataset.append((text, entities))\n",
    "\n",
    "output_csv_path = '/Users/pradeepsain/Downloads/TextFromImages.csv'\n",
    "\n",
    "header = ['Text', 'Start', 'End', 'Label']\n",
    "\n",
    "with open(output_csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for entry in dataset:\n",
    "        text = entry[0]\n",
    "        entities = entry[1]\n",
    "\n",
    "        for entity in entities:\n",
    "            start, end, label = entity\n",
    "            writer.writerow([text, start, end, label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a100737-a296-493e-a22a-e5dc3974a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.training.example import Example\n",
    "\n",
    "# # nlp = spacy.blank(\"en\")\n",
    "# # nlp = spacy.load(\"en_core_web_sm\")\n",
    "# # labels = [\"BG READING\", \"UNITS\", \"DATE\", \"TIME\", \"TIMESTAMP\", \"BG TEXT\"]\n",
    "# # ner = nlp.add_pipe(\"ner\", config={\"labels\": labels})\n",
    "# # nlp.add_pipe(\"ner\", config={\"labels\": [\"BG READING\", \"UNITS\",\"DATE\",\"TIME\",\"TIMESTAMP\",\"BG TEXT\"]})\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# ner = nlp.get_pipe(\"ner\")\n",
    "# custom_labels = [\"BG READING\", \"UNITS\", \"DATE\", \"TIME\", \"TIMESTAMP\", \"BG TEXT\"]\n",
    "# for label in custom_labels:\n",
    "#     ner.add_label(label)\n",
    "\n",
    "# TRAIN_DATA = output_csv_path\n",
    "\n",
    "# optimizer = nlp.begin_training()\n",
    "# for epoch in range(10):\n",
    "#     random.shuffle(TRAIN_DATA)\n",
    "#     for texts, annotations in TRAIN_DATA:\n",
    "#         example = Example.from_dict(nlp.make_doc(texts), annotations)\n",
    "#         nlp.update([example], drop=0.5, losses={})\n",
    "\n",
    "# nlp.to_disk(\"/Users/pradeepsain/Downloads/trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71f38193-e9ba-4847-a04d-1197bb30afc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.training.example import Example\n",
    "\n",
    "# nlp1 = spacy.blank(\"en\")\n",
    "\n",
    "# labels = [\"BG READING\", \"UNITS\", \"DATE\", \"TIME\", \"TIMESTAMP\", \"BG TEXT\"]\n",
    "# nlp1.add_pipe(\"ner\", config={\"labels\": labels})\n",
    "\n",
    "# TRAIN_DATA = pd.read_csv(output_csv_path)\n",
    "# # TRAIN_DATA = json.loads(json_file_path)\n",
    "\n",
    "# optimizer = nlp1.begin_training()\n",
    "# for epoch in range(10):\n",
    "#     random.shuffle(TRAIN_DATA)\n",
    "#     for texts, annotations in TRAIN_DATA:\n",
    "#         example = Example.from_dict(nlp1.make_doc(texts), annotations)\n",
    "#         nlp1.update([example], drop=0.5, losses={})\n",
    "\n",
    "# nlp1.to_disk(\"/Users/pradeepsain/Downloads/trained_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8553662-4ef8-4baf-9c58-e449d17af215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.training.example import Example\n",
    "# import json\n",
    "# import random\n",
    "\n",
    "# nlp1 = spacy.blank(\"en\")\n",
    "\n",
    "# labels = [\"BG READING\", \"UNITS\", \"DATE\", \"TIME\", \"TIMESTAMP\", \"BG TEXT\"]\n",
    "\n",
    "# nlp1.add_pipe(\"ner\", config={\"labels\": labels})\n",
    "\n",
    "# with open(json_file_path, \"r\") as json_file:\n",
    "#     TRAIN_DATA = json.load(json_file)\n",
    "\n",
    "# optimizer = nlp1.begin_training()\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     random.shuffle(TRAIN_DATA)\n",
    "#     losses = {}\n",
    "#     for texts, annotations in TRAIN_DATA:\n",
    "#         example = Example.from_dict(nlp1.make_doc(texts), annotations)\n",
    "        \n",
    "#         nlp1.update([example], drop=0.5, losses=losses)\n",
    "    \n",
    "#     print(\"Epoch:\", epoch, \"Losses:\", losses)\n",
    "\n",
    "# nlp1.to_disk(\"/Users/pradeepsain/Downloads/trained_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a06633c-2316-4891-acc1-bdaf53e010ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Labels specified in your code:\", labels)\n",
    "# # print(\"Labels specified in spaCy configuration:\", nlp1.get_pipe(\"ner\").labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13707f61-42f4-4c80-aa51-b7e0197b1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp3 = spacy.blank(\"en\")\n",
    "\n",
    "# labels = [\"BG READING\", \"UNITS\", \"DATE\", \"TIME\", \"TIMESTAMP\", \"BG TEXT\"]\n",
    "# # ner3 = nlp3.add_pipe(\"ner\", config={\"labels\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67d3e6b2-d8c1-4ca7-a1b6-06d8beba9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.pipeline.textcat import single_label_cnn_config\n",
    "# nlp=spacy.load(\"en_core_web_sm\")\n",
    "# config = Config().from_str(single_label_cnn_config)\n",
    "# if \"textcat\" not in nlp.pipe_names:\n",
    "#      nlp.add_pipe('textcat', config=config, last=True)\n",
    "\n",
    "# nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6784b3cf-7e0c-4609-827b-0613211b3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# if \"textcat\" not in nlp.pipe_names:\n",
    "#      nlp.add_pipe('textcat', config=single_label_cnn_config, last=True)\n",
    "# textcat = nlp.get_pipe('textcat')\n",
    "# textcat.add_label(\"pos\")\n",
    "# textcat.add_label(\"neg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a022edf2-fd0a-436f-a440-6a4c9e5881bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "\n",
    "df = pd.read_csv(output_csv_path)\n",
    "\n",
    "nlp = spacy.blank('en') \n",
    "\n",
    "def create_training_data(df):\n",
    "    training_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        text = row['Text']\n",
    "        start = row['Start']\n",
    "        end = row['End']\n",
    "        label = row['Label']\n",
    "\n",
    "        entities = [(start, end, label)]\n",
    "\n",
    "        example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
    "        training_data.append(example)\n",
    "    return training_data\n",
    "\n",
    "training_data = create_training_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8bb1e545-50f9-4fae-b881-a612d32d4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = nlp.add_pipe('ner')\n",
    "\n",
    "unique_labels = df['Label'].unique()\n",
    "for label in unique_labels:\n",
    "    ner.add_label(label)\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "for example in training_data:\n",
    "    nlp.update([example], drop=0.5)\n",
    "\n",
    "nlp.to_disk('/Users/pradeepsain/Downloads/ner_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5081ec3a-7470-48e7-99b8-6111e6492186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import spacy\n",
    "# from spacy.training.example import Example\n",
    "\n",
    "# # Load CSV data\n",
    "# df = pd.read_csv(output_csv_path)\n",
    "\n",
    "# nlp = spacy.blank('en')\n",
    "\n",
    "# def create_training_data(df):\n",
    "#     training_data = []\n",
    "#     for _, row in df.iterrows():\n",
    "#         text = row['text']\n",
    "#         annotations = row['annotation']  #.split('|')\n",
    "#         entities = []\n",
    "#         for annotation in annotations:\n",
    "#             entity, label = annotation   #.split('|')\n",
    "#             start = text.find(entity)\n",
    "#             end = start + len(entity)\n",
    "#             entities.append((start, end, label))\n",
    "#         example = Example.from_dict(nlp.make_doc(text), {\"entities\": entities})\n",
    "#         training_data.append(example)\n",
    "#     return training_data\n",
    "\n",
    "# training_data = create_training_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c50b6b-ae23-4b86-8c3e-fd301aa6b044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c28f4-5fb8-49a3-8d44-a2cf74f7a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f2e364-4cf3-4f6a-9496-94ada2d74640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6f58fe-8f40-4279-8eda-24291361256a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a00de5-1cc9-42a5-810b-e989800eb1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "41c753ae-bde3-4640-af60-1f2f1fc87542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Pod Dashboard Sep 26 Basal LAST BOLUS Date/Time change occurred X No Active Pod SET UP NEW POD HO 84 12:14 PM Bolus C Pod Info LAST BG 372 mg/dL Today Right Now 0 No Pod Dashboard Sep 26 Basal LAST BOLUS Date / Time change occurred X No Active Pod SET UP NEW POD HO 84 12:14 PM Bolus C Pod Info LAST BG 372 mg / dL Today Right Now 0\n",
      "Detected Readings: []\n",
      "Detected units: []\n",
      "Detected date: []\n",
      "Detected time: []\n",
      "Detected timestamp: []\n",
      "Detected bgText: []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp_trained = spacy.load(\"/Users/pradeepsain/Downloads/ner_model\")\n",
    "\n",
    "with open('/Users/pradeepsain/Downloads/dash_img/readingImg_6.jpg', 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "image = vision_v1.Image(content=content)\n",
    "image_context = vision_v1.ImageContext(language_hints=[\"en\"])\n",
    "\n",
    "response = client.text_detection(image=image, image_context=image_context)\n",
    "extracted_text = response.text_annotations\n",
    "\n",
    "cleaned_text = \"\"\n",
    "for text in extracted_text:\n",
    "    text_description = text.description\n",
    "    if isinstance(text_description, str):\n",
    "        cleaned_text += re.sub(r'[^\\w\\s./:]', '', text_description) + \" \"\n",
    "\n",
    "one_line_text = \" \".join(cleaned_text.split())\n",
    "\n",
    "print(one_line_text)\n",
    "\n",
    "doc = nlp_trained(one_line_text)\n",
    "\n",
    "\n",
    "readings = [ent.text for ent in doc.ents if ent.label_ == \"BG READING\"]\n",
    "units = [ent.text for ent in doc.ents if ent.label_ == \"UNITS\"]\n",
    "date = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
    "time = [ent.text for ent in doc.ents if ent.label_ == \"TIME\"]\n",
    "timestamp = [ent.text for ent in doc.ents if ent.label_ == \"TIMESTAMP\"]\n",
    "bgText = [ent.text for ent in doc.ents if ent.label_ == \"BG TEXT\"]\n",
    "\n",
    "print(\"Detected Readings:\", readings)\n",
    "print(\"Detected units:\", units)\n",
    "print(\"Detected date:\", date)\n",
    "print(\"Detected time:\", time)\n",
    "print(\"Detected timestamp:\", timestamp)\n",
    "print(\"Detected bgText:\", bgText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "88ed7771-7730-4861-8314-857919caf2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Readings: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Detected Readings:\", [ent.text for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf5b0d5-4d36-43ad-b129-3acffeb3cc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330480d2-d407-4c14-badf-35b741628346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
