{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c862c430-3a19-422b-9861-73002676f6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43cd864f-cdf8-44dc-a49f-c9417818760c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCU-CHEK*\n",
      "Guide\n",
      "9:38 5/11/20\n",
      "5.8\n",
      "mmol/L\n",
      "738 4.7 mmol\n",
      "f has\n",
      "EEL OK:\n",
      "Hee\n",
      "ow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image = Image.open('/Users/pradeepsain/Downloads/accuChek_7.jpeg')\n",
    "\n",
    "text = pytesseract.image_to_string(image)\n",
    "text = pytesseract.image_to_string(image, lang='eng', config='--psm 6')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72eff3ff-2248-4e7e-a56f-cd2d6aec0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = text.split('\\n')\n",
    "data = []\n",
    "for line in lines:\n",
    "    values = line.split(' ')\n",
    "    data.append(values)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['1', '2', '3'])\n",
    "\n",
    "df.to_csv('output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba3a899d-f29a-4cc1-8186-25fe9c8ea25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1        2     3\n",
      "0   ACCU-CHEK*     None  None\n",
      "1        Guide     None  None\n",
      "2         9:38  5/11/20  None\n",
      "3          5.8     None  None\n",
      "4       mmol/L     None  None\n",
      "5          738      4.7  mmol\n",
      "6            f      has  None\n",
      "7          EEL      OK:  None\n",
      "8          Hee     None  None\n",
      "9           ow     None  None\n",
      "10                 None  None\n"
     ]
    }
   ],
   "source": [
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1962b89b-0f9f-444a-92b8-29bea9ffe696",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_pattern = r'\\d{2}:\\d{2} \\d{2}-\\d{2}-\\d{2}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e475a39e-28d1-49eb-8df6-7bc10598524a",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = re.findall(timestamp_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7807b1c1-c8d7-4762-927b-b5c276bb31bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp in timestamps:\n",
    "    print(\"Timestamp:\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4d683835-d880-4167-b943-962eca74e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for timestamp in timestamps:\n",
    "    background_value = extract_background_value(text, timestamp)\n",
    "    data[timestamp] = background_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e7904a7-1b7e-48fc-821d-b81eeb8109bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp, bg_value in data.items():\n",
    "    print(\"Timestamp:\", timestamp, \"Background Value:\", bg_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fee8af1f-57ad-4ceb-9c70-350015afb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_pattern = r'\\d{1,2}:\\d{2} \\d{1,2}/\\d{1,2}/\\d{2,4}'\n",
    "value_pattern = r'\\d+\\.\\d+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b431b594-8a1e-44ef-9a83-ed178ceb2772",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = re.findall(timestamp_pattern, text)\n",
    "values = re.findall(value_pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13626741-31ab-4ad5-a51d-6df28740a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 9:38 5/11/20\n"
     ]
    }
   ],
   "source": [
    "for timestamp in timestamps:\n",
    "    print(\"Timestamp:\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58074cde-73f8-4def-9f33-c94fe2070d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: 5.8\n",
      "Value: 4.7\n"
     ]
    }
   ],
   "source": [
    "for value in values:\n",
    "    print(\"Value:\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0814fbf8-1457-4b60-ab06-63246c3476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2 = pd.DataFrame({'Timestamp': timestamps, 'Value': values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a54eefe-82e8-4bdc-89e5-36434f5874f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_value = r'(\\d+\\.\\d+)\\s+(mmol/L|mg/dl)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed7bbdaf-e58d-4d40-89f7-086c0ce923ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = re.findall(pattern_value, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1e72f9-58d2-49f6-9e4f-5a825f8df16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for timestamp in timestamps:\n",
    "    print(\"Timestamp:\", timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fba1530-57eb-4314-8716-29e7652db859",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_for_all_values = r'(\\d+:\\d+\\s+\\d+/\\d+/\\d+)\\s+(\\d+\\.\\d+)\\s+(mmol/L|mg/dl)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8938c17-ebd0-41ce-9bca-3f1e52280a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_values = re.findall(pattern_for_all_values, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "31f9588a-4164-40b4-9918-60d01867e438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCUCHEK\n",
      "Guide\n",
      "9:38 5/11/20\n",
      "5.8\n",
      "mmol/L\n",
      "738 4.7 mmol\n",
      "f has\n",
      "EEL OK:\n",
      "Hee\n",
      "ow\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "cleaned_text = re.sub(r'[^\\w\\s./:]', '', text)\n",
    "\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8fb818b7-ed48-4a1a-ba24-7df9153bf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9:38 5/11/20', 'CARDINAL'), ('5.8', 'CARDINAL'), ('738 4.7', 'CARDINAL'), ('EEL', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with spaCy's NER model\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Extract named entities and date patterns\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Print the extracted entities\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bac51b9a-d571-4373-8bc0-144b8f55ec17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('9:38 5/11/20', 'CARDINAL'), ('5.8', 'CARDINAL'), ('738 4.7', 'CARDINAL'), ('EEL', 'ORG')]\n",
      "['mmol', 'mmol']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Define a function to identify units\n",
    "def get_units(doc):\n",
    "    units = []\n",
    "    for token in doc:\n",
    "        if token.text in (\"mmol/L\", \"mmol\", \"mg/dL\"):\n",
    "            units.append(token.text)\n",
    "    return units\n",
    "\n",
    "# Extract named entities and units\n",
    "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "units = get_units(doc)\n",
    "\n",
    "# Print the extracted entities and units\n",
    "print(entities)\n",
    "print(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "daf0b179-127b-4146-b9b6-eebf9c44e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_for_all_values = pd.DataFrame({'Timestamp': [all_values[0] for value in all_values], 'Value': [all_values[1] for value in all_values], 'Unit': [all_values[2] for value in all_values]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ee6eb7a-b3ff-44ea-87e3-4bb9f39682e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytesseract.pytesseract import Output \n",
    "# image_2 = Image.open('/Users/pradeepsain/Downloads/accuChek_7.jpeg')\n",
    "# data = pytesseract.image_to_osd(image_2, output_type=Output.DICT)\n",
    "\n",
    "# total_font_size = sum(data['fontsize'])\n",
    "# average_font_size = total_font_size / len(data['text'])\n",
    "\n",
    "# font_size_threshold = 2.0\n",
    "\n",
    "# filtered_text = ''\n",
    "\n",
    "# # Iterate through the detected text and filter by font size\n",
    "# for i in range(len(data['text'])):\n",
    "#     font_size = int(data['fontsize'][i])\n",
    "#     if font_size >= average_font_size * font_size_threshold:\n",
    "#         filtered_text += data['text'][i] + ' '\n",
    "\n",
    "# print(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "68a6cc4c-4520-46d5-ade5-208f95b246e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradeepsain/Library/Python/3.8/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    # Open the image using Pillow (PIL)\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Use pytesseract to extract text from the image\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ef0dcb7-3f56-4e85-a7f0-02d8351dca71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: ACCU, POS: PROPN, Dependency: compound\n",
      "Token: -, POS: PUNCT, Dependency: punct\n",
      "Token: CHEK, POS: PROPN, Dependency: nmod\n",
      "Token: *, POS: SYM, Dependency: punct\n",
      "Token: \n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: Guide, POS: PROPN, Dependency: compound\n",
      "Token: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n",
      "Token: 9:38, POS: NUM, Dependency: nummod\n",
      "Token: 5/11/20, POS: NUM, Dependency: ROOT\n",
      "Token: \n",
      "\n",
      ", POS: SPACE, Dependency: dep\n"
     ]
    }
   ],
   "source": [
    "def analyze_text_with_spacy(text):\n",
    "    # Process the text using spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Perform any desired analysis on 'doc'\n",
    "    # For example, you can iterate through tokens and extract information\n",
    "    for token in doc:\n",
    "        print(f\"Token: {token.text}, POS: {token.pos_}, Dependency: {token.dep_}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_path = \"/Users/pradeepsain/Downloads/accuChek_7.jpeg\"  # Replace with the path to your image\n",
    "    extracted_text = extract_text_from_image(image_path)\n",
    "\n",
    "    # Perform any filtering or preprocessing of 'extracted_text' here if needed\n",
    "\n",
    "    analyze_text_with_spacy(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb3951-e9c6-4d5e-9ec1-3abf625fca69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5375460-f573-43de-8307-ff723ba1d22c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a14b3-66c9-4f3a-8eb2-c72952756cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350fc3f-edca-4012-8aec-2b1d4a9d6e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6316d-610f-43f1-8abb-6f49e62f198b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b996c3-243c-45e4-b610-cc8ddf739aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "494c342c-9b7f-4c9d-ab56-88fa22d9d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import vision\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import vision_v1\n",
    "from google.protobuf import json_format\n",
    "\n",
    "# Replace with your service account key file path\n",
    "key_path = '/Users/pradeepsain/Downloads/KEY_FILE.json'\n",
    "\n",
    "# Initialize the client\n",
    "credentials = service_account.Credentials.from_service_account_file(key_path)\n",
    "client = vision.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "# Load the image\n",
    "with open('/Users/pradeepsain/Downloads/accuChek_7.jpeg', 'rb') as image_file:\n",
    "    content = image_file.read()\n",
    "\n",
    "# Create an image object with the language hint set to English\n",
    "image = vision_v1.Image(content=content)\n",
    "image_context = vision_v1.ImageContext(language_hints=[\"en\"])  # Specify English as the language\n",
    "\n",
    "# Perform text detection\n",
    "# image = vision.Image(content=content)\n",
    "# response = client.text_detection(image=image)\n",
    "\n",
    "response = client.text_detection(image=image, image_context=image_context)\n",
    "extracted_text = response.text_annotations\n",
    "\n",
    "# Extract text\n",
    "# texts = response.text_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d441153-947a-4c08-a70e-b581d6572479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCU-CHEK\n",
      "Guide\n",
      "9:38\n",
      "5/11/20\n",
      "5.8\n",
      "mmol/L\n",
      "Add Comment\n",
      "7:38\n",
      "4.7 mmol/L\n",
      "OK\n",
      "O\n",
      "ACCU\n",
      "-\n",
      "CHEK\n",
      "Guide\n",
      "9:38\n",
      "5/11/20\n",
      "5.8\n",
      "mmol\n",
      "/\n",
      "L\n",
      "Add\n",
      "Comment\n",
      "7:38\n",
      "4.7\n",
      "mmol\n",
      "/\n",
      "L\n",
      "OK\n",
      "O\n"
     ]
    }
   ],
   "source": [
    "for text in extracted_text:\n",
    "    print(text.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "14c71e2f-185a-41f4-ac6b-9d3afa62e6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCUCHEK\n",
      "Guide\n",
      "9:38\n",
      "5/11/20\n",
      "5.8\n",
      "mmol/L\n",
      "Add Comment\n",
      "7:38\n",
      "4.7 mmol/L\n",
      "OK\n",
      "O ACCU  CHEK Guide 9:38 5/11/20 5.8 mmol / L Add Comment 7:38 4.7 mmol / L OK O \n"
     ]
    }
   ],
   "source": [
    "cleaned_text = \"\"\n",
    "for text in extracted_text:\n",
    "    text_description = text.description\n",
    "\n",
    "    # Ensure text_description is a string\n",
    "    if isinstance(text_description, str):\n",
    "        # Apply your text cleaning logic here\n",
    "        # For example, remove special characters\n",
    "        cleaned_text += re.sub(r'[^\\w\\s./:]', '', text_description) + \" \"\n",
    "\n",
    "# Print the cleaned text\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4d31a091-77ae-403f-83ab-ca55648ee9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCUCHEK Guide 9:38 5/11/20 5.8 mmol/L Add Comment 7:38 4.7 mmol/L OK O ACCU CHEK Guide 9:38 5/11/20 5.8 mmol / L Add Comment 7:38 4.7 mmol / L OK O\n"
     ]
    }
   ],
   "source": [
    "text_in_one_line = \" \".join(cleaned_text.split())\n",
    "print(text_in_one_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191b72e-8597-4b98-8a49-6d1a8a957a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072a2103-1912-4411-8355-b9a8097c1a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b3cd2d-7d8d-4d0d-a04e-8c681d55f840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d4969d-7cc8-4c17-a5b8-dab6c33b87be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bd7b12-45c5-42b0-9ef6-4c7537731cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef509b92-fde0-4f57-89a3-bc634ac5a903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89400a78-b447-459d-88ae-d8da4b554447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d8c1c-dba6-4298-8251-8b1dcfdc5566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b86d90-b6ae-49dd-ae2a-4cb0de8bc485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b70a4e-d028-41ec-a8c9-8c45a07e590d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a9a1f13c-c897-46fe-9697-cc9d1edb2021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Readings:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the text with spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Initialize variables to store extracted information\n",
    "readings = []\n",
    "\n",
    "# Iterate through entities in the processed text\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"TIME\" or ent.label_ == \"DATE\" or ent.label_ == \"QUANTITY\":\n",
    "        readings.append(ent.text)\n",
    "\n",
    "# Extracted readings\n",
    "print(\"Extracted Readings:\")\n",
    "print(\"\\n\".join(readings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901a0f9-3710-490a-8cc8-696820f73899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "db60411e-2e29-4fcc-a209-1647a2887b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9:38\n",
      "Date: 5/11/20\n",
      "mmol/L: 4.7 mmol/L\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Regular expressions to match time, date, and mmol/L\n",
    "time_pattern = r'\\d+:\\d+'\n",
    "date_pattern = r'\\d+/\\d+/\\d+'\n",
    "mmol_pattern = r'\\d+\\.\\d+ mmol/L'\n",
    "\n",
    "# Find matches using the regular expressions\n",
    "time_matches = re.findall(time_pattern, cleaned_text)\n",
    "date_matches = re.findall(date_pattern, cleaned_text)\n",
    "mmol_matches = re.findall(mmol_pattern, cleaned_text)\n",
    "\n",
    "# Extract the first match (assuming there's only one match of each type)\n",
    "time_value = time_matches[0] if time_matches else None\n",
    "date_value = date_matches[0] if date_matches else None\n",
    "mmol_value = mmol_matches[0] if mmol_matches else None\n",
    "\n",
    "# Print the extracted information\n",
    "print(\"Time:\", time_value)\n",
    "print(\"Date:\", date_value)\n",
    "print(\"mmol/L:\", mmol_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec8206a4-48c9-4345-8f9d-cbe42cef6d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: None\n",
      "Date: None\n",
      "mmol/L: None\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a spaCy model (you might need to use a custom-trained model if available)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Initialize variables to store extracted information\n",
    "time_value = None\n",
    "date_value = None\n",
    "mmol_value = None\n",
    "\n",
    "# Iterate through the entities recognized by spaCy\n",
    "for entity in doc.ents:\n",
    "    if entity.label_ == \"TIME\":\n",
    "        time_value = entity.text\n",
    "    elif entity.label_ == \"DATE\":\n",
    "        date_value = entity.text\n",
    "    elif entity.label_ == \"UNITS\":\n",
    "        mmol_value = entity.text\n",
    "\n",
    "# Print the extracted information\n",
    "print(\"Time:\", time_value)\n",
    "print(\"Date:\", date_value)\n",
    "print(\"mmol/L:\", mmol_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "67d6dc7b-91a2-48ed-b738-9c081b802a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readings: []\n",
      "Dates: ['5/11/20']\n",
      "Times: ['9:38', '7:38']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Initialize spaCy with English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Define a pattern for 'READING' entities using spaCy's Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Wrap the pattern in a list\n",
    "# matcher.add(\"READING_PATTERN\", [[{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True}, {\"LIKE_NUM\": True}, {\"TEXT\": {\"in\": [\"mmol/L\", \"mg/dl\"]}}]])\n",
    "# reading_pattern = [{\"LIKE_NUM\": True}, {\"IS_PUNCT\": True, \"OP\": \"?\"}, {\"LIKE_NUM\": True}, {\"TEXT\": {\"in\": [\"mmol/L\", \"mg/dl\"]}}]\n",
    "# matcher.add(\"READING_PATTERN\", [reading_pattern])\n",
    "\n",
    "reading_pattern = [\n",
    "    {\"LIKE_NUM\": True},  # Optional numeric part\n",
    "    {\"IS_PUNCT\": True},  # Optional punctuation\n",
    "    {\"LIKE_NUM\": True},\n",
    "    {\"TEXT\": {\"in\": [\"mmol/L\", \"mg/dl\", \"mg/dL\"]}},\n",
    "]\n",
    "\n",
    "matcher.add(\"READING_PATTERN\", [reading_pattern])\n",
    "\n",
    "# Process the text with spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Extract entities based on the pattern\n",
    "readings = []\n",
    "for match_id, token_ids in matcher(doc):\n",
    "    start, end = token_ids[0], token_ids[-1] + 1\n",
    "    reading = doc[start:end].text\n",
    "    readings.append(reading)\n",
    "\n",
    "# Extract date (MM/DD/YY or MM/DD/YYYY format) using regular expression\n",
    "date_pattern = r'\\d{1,2}/\\d{1,2}/\\d{2,4}'\n",
    "dates = re.findall(date_pattern, text)\n",
    "\n",
    "# Extract time (HH:MM format) using regular expression\n",
    "time_pattern = r'\\d{1,2}:\\d{2}'\n",
    "times = re.findall(time_pattern, text)\n",
    "\n",
    "# Print the extracted values\n",
    "print(\"Readings:\", readings)\n",
    "print(\"Dates:\", dates)\n",
    "print(\"Times:\", times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ed375a70-f7cd-4028-88be-1e053b42579e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pradeepsain/Library/Python/3.8/lib/python/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"ACCUCHEK\n",
      "Guide\n",
      "9:38\n",
      "5/11/20\n",
      "5.8\n",
      "mmol/L\n",
      "Add Comment...\" with entities \"[(23, 27, 'READING')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[E022] Could not find a transition with the name 'O' in the NER model.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, annotations \u001b[38;5;129;01min\u001b[39;00m TRAIN_DATA:\n\u001b[1;32m     16\u001b[0m     example \u001b[38;5;241m=\u001b[39m Example\u001b[38;5;241m.\u001b[39mfrom_dict(nlp\u001b[38;5;241m.\u001b[39mmake_doc(text), annotations)\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Process the text\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# text = \u001b[39;00m\n\u001b[1;32m     21\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(cleaned_text)\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/language.py:1186\u001b[0m, in \u001b[0;36mLanguage.update\u001b[0;34m(self, examples, _, drop, sgd, losses, component_cfg, exclude, annotates)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, proc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline:\n\u001b[1;32m   1184\u001b[0m     \u001b[38;5;66;03m# ignore statements are used here because mypy ignores hasattr\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(proc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1186\u001b[0m         \u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msgd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sgd \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1188\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1189\u001b[0m             name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude\n\u001b[1;32m   1190\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proc, ty\u001b[38;5;241m.\u001b[39mTrainableComponent)\n\u001b[1;32m   1191\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mis_trainable\n\u001b[1;32m   1192\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1193\u001b[0m         ):\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/transition_parser.pyx:410\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.update\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/transition_parser.pyx:670\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser._init_gold_batch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/_parser_internals/ner.pyx:296\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.init_gold\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/_parser_internals/ner.pyx:60\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoGold.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/_parser_internals/ner.pyx:88\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.create_gold_state\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Library/Python/3.8/lib/python/site-packages/spacy/pipeline/_parser_internals/ner.pyx:200\u001b[0m, in \u001b[0;36mspacy.pipeline._parser_internals.ner.BiluoPushDown.lookup_transition\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E022] Could not find a transition with the name 'O' in the NER model.\""
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training.example import Example\n",
    "# Define a spaCy NER model and add a custom entity type for \"READING\"\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.add_pipe(\"ner\")\n",
    "ner.add_label(\"READING\")\n",
    "\n",
    "TRAIN_DATA = [\n",
    "    (cleaned_text, {\"entities\": [(23, 27, \"READING\")]}),\n",
    "    (cleaned_text, {\"entities\": [(18, 22, \"READING\")]}),\n",
    "    # Add more annotated examples as needed\n",
    "]\n",
    "\n",
    "# Train the NER model\n",
    "for text, annotations in TRAIN_DATA:\n",
    "    example = Example.from_dict(nlp.make_doc(text), annotations)\n",
    "    nlp.update([example], drop=0.5, losses={})\n",
    "\n",
    "# Process the text\n",
    "# text = \n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Extract entities (readings)\n",
    "readings = [ent.text for ent in doc.ents if ent.label_ == \"READING\"]\n",
    "\n",
    "# Extract date and time (for simplicity, we're not extracting units in this example)\n",
    "for token in doc:\n",
    "    if token.like_time:\n",
    "        time = token.text\n",
    "    elif token.like_date:\n",
    "        date = token.text\n",
    "\n",
    "# Print the extracted values\n",
    "print(\"Time:\", time)\n",
    "print(\"Date:\", date)\n",
    "print(\"Readings:\", readings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "370cc4e1-c08d-463f-91cd-27fc1b7b4a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Time: None\n",
      "Detected Unit: None\n",
      "Detected Reading: None\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process the filtered text using spaCy\n",
    "doc = nlp(cleaned_text)\n",
    "\n",
    "# Initialize variables to store detected time, unit, and reading\n",
    "detected_time = None\n",
    "detected_unit = None\n",
    "detected_reading = None\n",
    "detected_date = None\n",
    "\n",
    "# Iterate through spaCy tokens to extract information\n",
    "for token in doc:\n",
    "    # Detect time in the format HH:MM\n",
    "    if token.like_time:\n",
    "        detected_time = token.text\n",
    "\n",
    "    if token.like_date:\n",
    "        detected_date = token.text\n",
    "        \n",
    "    # Detect units (mg/dl or mmo/l)\n",
    "    if token.text.lower() in [\"mg/dl\", \"mmo/l\",\"mg/dL\", \"mmol/L\"]:\n",
    "        detected_unit = token.text\n",
    "    \n",
    "    # Detect readings (e.g., numeric values)\n",
    "    if token.is_digit:\n",
    "        detected_reading = token.text\n",
    "\n",
    "# Print the detected information\n",
    "print(\"Detected Time:\", detected_time)\n",
    "print(\"Detected Unit:\", detected_unit)\n",
    "print(\"Detected Reading:\", detected_reading)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75cf9b7-e84b-4942-a61c-72dc45c5130e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc062d-a781-482e-9211-744ffe957725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6d0b55-ff4a-456c-91c2-a4d0729d4138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3300676c-70e2-45d8-9f4b-0facb9fb8811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372fd76b-9161-4d4a-9657-d679aa85efaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
